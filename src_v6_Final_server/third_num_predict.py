# for main class 1:
# subclass: 1-18, 20, 21, 97
# This is for subclass 1
# [('1', 569), ('3', 25), ('5', 23), ('99', 20), ('2', 11), ('98', 2), ('9', 1)]
# This is for subclass 2
# [('1', 193), ('4', 27), ('7', 26), ('99', 8), ('3', 8), ('5', 5), ('2', 1)]
# This is for subclass 3
# [('1', 153), ('3', 36), ('7', 36), ('5', 25), ('99', 24), ('8', 12), ('98', 2), ('4', 2), ('2', 1)]
# This is for subclass 4
# [('3', 72), ('1', 44), ('4', 9), ('99', 4), ('7', 3), ('9', 3), ('5', 1)]
# This is for subclass 5
# [('1', 94), ('3', 21), ('5', 13), ('99', 7), ('8', 3), ('98', 2), ('4', 1)
# This is for subclass 6
# [('5', 376), ('2', 17), ('99', 10), ('1', 7), ('3', 2)]
# This is for subclass 7
# [('1', 32), ('99', 19), ('2', 10), ('3', 8), ('7', 5), ('6', 1)]
# This is for subclass 8
# [('1', 60), ('4', 43), ('3', 22), ('98', 11), ('5', 6), ('7', 6), ('99', 5), ('2', 4)]
# This is for subclass 9
# [('3', 86)]
# This is for subclass 10
# [('3', 32), ('2', 5), ('9', 4), ('5', 1)]
# This is for subclass 11
# [('1', 133), ('2', 6)]
# This is for subclass 12
# [('98', 18), ('1', 12), ('99', 5), ('7', 5), ('2', 4), ('5', 1)]
# This is for subclass 13
# [('11', 112), ('12', 18), ('99', 2)]
# This is for subclass 14
# [('13', 155), ('11', 76), ('14', 53), ('19', 38), ('12', 24), ('99', 22), ('18', 21), ('15', 16), ('16', 9), ('17', 5), ('21', 4), ('20', 1)]
# This is for subclass 15
# [('1', 47)]
# This is for subclass 16
# [('3', 23), ('1', 21), ('5', 3)]
# This is for subclass 17
# [('4', 50), ('1', 44), ('7', 37), ('99', 8), ('2', 2), ('5', 2), ('98', 1)]
# This is for subclass 18
# [('1', 38), ('6', 23)]
# This is for subclass 20
# [('4', 7), ('9', 2), ('1', 1)]
# This is for subclass 21
# [('99', 6), ('4', 5), ('3', 4), ('1', 1), ('98', 1)]
# This is for subclass 97
# [('1', 17)]


# for main class 2:
# subclass: 1-10
# This is for subclass 1
# [('1', 1238), ('2', 71), ('3', 38), ('4', 2)]
# This is for subclass 2
# [('1', 77)]
# This is for subclass 3
# [('1', 779), ('2', 114), ('3', 52)]
# This is for subclass 4
# [('1', 537), ('2', 262), ('99', 62)]
# This is for subclass 5
# [('1', 526)]
# This is for subclass 6
# [('1', 171), ('99', 5)]
# This is for subclass 7
# [('7', 1214), ('11', 1165), ('1', 910), ('4', 247), ('8', 217), ('13', 191), ('10', 145), ('2', 85), ('12', 59), ('6', 43), ('9', 18), ('3', 14), ('14', 4)]
# This is for subclass 8
# [('1', 158), ('4', 55), ('2', 26), ('3', 20)]
# This is for subclass 9
# [('1', 5)]
# This is for subclass 10
# [('1', 7)]


# for main class 3:
# subclass: 1-8, 11
# This is for subclass 1
# [('3', 660), ('1', 447), ('26', 330), ('4', 175), ('21', 137), ('11', 100), ('13', 64), ('2', 52), ('22', 41), ('27', 32), ('6', 15), ('12', 8), ('7', 8), ('31', 3), ('30', 3), ('8', 3), ('5', 1)]
# This is for subclass 2
# [('1', 799), ('2', 77)]
# This is for subclass 3
# [('2', 15), ('1', 8)]
# This is for subclass 4
# [('21', 231), ('19', 175), ('24', 138), ('23', 85), ('11', 80), ('22', 76), ('25', 57), ('16', 41), ('14', 27), ('17', 26), ('13', 17), ('15', 6), ('18', 1)]
# This is for subclass 5
# [('1', 342), ('4', 181), ('2', 95), ('3', 50), ('99', 21), ('5', 12)]
# This is for subclass 6
# [('4', 640), ('1', 342), ('3', 227), ('5', 18), ('2', 2)]
# This is for subclass 7
# [('1', 24)]
# This is for subclass 8
# [('1', 14)]
# This is for subclass 11
# [('1', 7)]

# for main class 4:
# subclass: 1,2,3,4,6,99
# This is for subclass 1
# [('1', 324), ('3', 95), ('2', 85), ('99', 60)]
# This is for subclass 2
# [('1', 409), ('3', 103), ('2', 69), ('99', 21)]
# This is for subclass 3
# [('1', 52), ('2', 37), ('3', 27), ('99', 26)]
# This is for subclass 4
# [('1', 62)]
# This is for subclass 6
# [('1', 112)]
# This is for subclass 99
# [('1', 48)]


# for main class 5:
# subclass: 1,2,3,4,5,99
# This is for subclass 1
# [('1', 114), ('3', 70), ('99', 26), ('2', 1)]
# This is for subclass 2
# [('1', 270)]
# This is for subclass 3
# [('1', 205), ('3', 49), ('4', 23), ('99', 10), ('2', 6)]
# This is for subclass 4
# [('99', 213), ('2', 51), ('3', 12), ('4', 9), ('1', 2)]
# This is for subclass 5
# [('1', 28)]
# This is for subclass 99
# [('1', 104)]


# for main class 6:
# subclass: 1-6
# This is for subclass 1
# [('1', 712), ('2', 2)]
# This is for subclass 2
# [('1', 87)]
# This is for subclass 3
# [('2', 261), ('4', 218), ('5', 101), ('3', 58), ('1', 52)]
# This is for subclass 4
# [('1', 38)]
# This is for subclass 5
# [('1', 123)]
# This is for subclass 6
# [('1', 14)]


import numpy as np
import tflearn
from sklearn.model_selection import train_test_split
import random
import pickle
import copy
import math
import sys
import tensorflow as tf
import os
import pdb


def predict_third_digit(test_functional_domain_encoding,test_esm_1b, first_digit, second_digit):
    if first_digit[0] == 0:
        class_0_result = []
        class_0_labels = [0]
        class_0_prob_list = []
        for i in range(len(first_digit)):
            class_0_result.append(0)
            class_0_prob_list.append([0.0])
        return class_0_result, class_0_labels, class_0_prob_list
    
    concat_digit = str(first_digit[0])+'.'+str(second_digit[0])
    detemined_list = ['6.1','6.2','6.4','6.5','6.6','5.2','5.5','5.99','4.4','4.6','4.99',
        '3.7','3.8','3.11','2.2','2.5','2.9','2.10', '1.15','1.20','1.97']

    if concat_digit in detemined_list:
        class_detemined_list_result = []
        class_detemined_list_labels = [1]
        class_detemined_list_prob_list = []
        for i in range(len(first_digit)):
            class_detemined_list_result.append(1)
            class_detemined_list_prob_list.append([0.0])
        return class_detemined_list_result, class_detemined_list_labels, class_detemined_list_prob_list
    
    if concat_digit =='1.9':
        class_detemined2_list_result = []
        class_detemined2_list_labels = [3]
        class_detemined2_list_prob_list = []
        for i in range(len(first_digit)):
            class_detemined2_list_result.append(3)
            class_detemined2_list_prob_list.append([0.0])
        return class_detemined2_list_result, class_detemined2_list_labels, class_detemined2_list_prob_list
    
    
    unique_label_dict={
    '1.1': [1,2,3,5,99],
    '1.2': [1,3,4,5,7,99],
    '1.3': [1,3,5,7,8,99],
    '1.4': [1,3,4],
    '1.5': [1,3,5,99],
    '1.6': [1,2,5,99],
    '1.7': [1,2,3,7,99],
    '1.8': [1,3,4,5,7,98,99],
    '1.10': [2,3],
    '1.11': [1,2],
    '1.12': [1,7,98,99],
    '1.13': [11,12],
    '1.14': list(range(11,20))+[99],
    '1.16': [1,3],
    '1.17': [1,4,7,99],
    '1.18': [1,6],
    '1.21': [4,99],
    '2.1': list(range(1,4)),
    '2.3': list(range(1,4)),
    '2.4': [1,2,99],
    '2.6': [1,99],
    '2.7': [1,2,3,4,6,7,8,9,10,11,12,13],
    '2.8': list(range(1,5)),
    '3.1': [1,2,3,4,6,7,11,12,13,21,22,26,27],
    '3.2': [1,2],
    '3.3': [1,2],
    '3.4': [11,13,14,15,16,17,19,21,22,23,24,25],
    '3.5': list(range(1,6)) + [99],
    '3.6': [1,3,4,5],
    '4.1': [1,2,3,99],
    '4.2': [1,2,3,99],
    '4.3': [1,2,3,99],
    '5.1': [1,3,99],
    '5.3': [1,2,3,4,99],
    '5.4': [2,3,4,99],
    '6.3': list(range(1,6))
    }
    unique_label = unique_label_dict[concat_digit]


    level=3
    DROPOUT=False
    MAX_LENGTH=1250
    DOMAIN=16306
    ESM_1B=1280
    LOAD=True
    batch_size=20000
    theta=0.0002
    model_name = '../database/DL_models/level_'+str(level)+'_class_'+str(first_digit[0])+'_subclass_'+str(second_digit[0])+'.ckpt'


    if len(np.shape(test_esm_1b))==1:
        test_functional_domain_encoding = np.reshape(test_functional_domain_encoding,(1,)+np.shape(test_functional_domain_encoding))
        test_esm_1b = np.reshape(test_esm_1b,(1,)+np.shape(test_esm_1b))



    #functions to generate variables, like weight and bias
    def weight_variable(shape):
        import math
        if len(shape)>2:
            weight_std=math.sqrt(2.0/(shape[0]*shape[1]*shape[2]))
        else:
            weight_std=0.01
        initial=tf.truncated_normal(shape,stddev=weight_std)
        return tf.Variable(initial,name='weights')
    
    def bias_variable(shape):
        initial=tf.constant(0.1,shape=shape)
        return tf.Variable(initial,name='bias')
    
    def weight_variable_2d_attention(shape):
        import math
        weight_std=1.0
        initial=tf.truncated_normal(shape,stddev=weight_std)
        return tf.Variable(initial,name='weights')
    
    def bias_variable_2d_attention(shape):
        initial=tf.constant(0.0,shape=shape)
        return tf.Variable(initial,name='bias')
    
    def weight_variable_3d_attention(shape):
        import math
        weight_std=1.0
        initial=tf.truncated_normal(shape,stddev=weight_std)
        return tf.Variable(initial,name='weights')
    
    def bias_variable_3d_attention(shape):
        initial=tf.constant(0.0,shape=shape)
        return tf.Variable(initial,name='bias')
    
    def conv2d(x,W):
        return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')
    def aver_pool2d(x,row,col):
        return tf.nn.avg_pool(x,ksize=[1,row,col,1],strides=[1,row,col,1],padding='SAME')
    def max_pool2d(x,row,col):
        return tf.nn.max_pool(x,ksize=[1,row,col,1],strides=[1,row,col,1],padding='SAME')
    
    
    sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=False,allow_soft_placement=True))

    def variable_summaries(var, name):
      """Attach a lot of summaries to a Tensor."""
      with tf.name_scope('summaries'):
        mean = tf.reduce_mean(var)
        tf.summary.scalar('mean/' + name, mean)
        with tf.name_scope('stddev'):
          stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
        tf.summary.scalar('sttdev/' + name, stddev)
        tf.summary.scalar('max/' + name, tf.reduce_max(var))
        tf.summary.scalar('min/' + name, tf.reduce_min(var))
        tf.summary.histogram(name, var)
    
    with tf.name_scope('placeholder'):
        y_=tf.placeholder(tf.float32,shape=[None,len(unique_label)])
        domain=tf.placeholder(tf.float32,shape=[None,DOMAIN])
        esm_1b=tf.placeholder(tf.float32,shape=[None,ESM_1B])
        keep_prob=tf.placeholder(tf.float32)
    

    with tf.name_scope('fine_tune_layers'):
        with tf.name_scope('functional_domain_layers'):
            with tf.name_scope('functional_domain_2d_attention'):
                domain_bn = tflearn.batch_normalization(domain)
                domain_after_2d_attention = domain_bn
            
            with tf.name_scope('functional_domain_fc_1'):
                w_dr1_domain=weight_variable([DOMAIN,256])
                b_dr1_domain=bias_variable([256])
                h_dr1_domain=tflearn.prelu(tf.matmul(domain_after_2d_attention,w_dr1_domain)+b_dr1_domain)
                h_dr1_domain=tflearn.batch_normalization(h_dr1_domain)
    

        with tf.name_scope('esm_1b_layers'):
            with tf.name_scope('esm_1b_2d_attention'):
                w_2d_attention_esm_1b = weight_variable_2d_attention([ESM_1B,ESM_1B])
                b_2d_attention_esm_1b = bias_variable_2d_attention([ESM_1B])
                esm_1b_bn = tflearn.batch_normalization(esm_1b)
                layer_attention_esm_1b = tf.matmul(esm_1b_bn,w_2d_attention_esm_1b)+b_2d_attention_esm_1b
                layer_attention_esm_1b_weights = tf.nn.sigmoid(layer_attention_esm_1b)   
                esm_1b_after_2d_attention = tf.multiply(esm_1b_bn,layer_attention_esm_1b_weights)
                esm_1b_after_2d_attention = tflearn.batch_normalization(esm_1b_after_2d_attention)

            
            with tf.name_scope('esm_1b_fc_1'):
                w_dr1_esm_1b=weight_variable([ESM_1B,256])
                b_dr1_esm_1b=bias_variable([256])
                h_dr1_esm_1b=tf.nn.relu(tf.matmul(esm_1b_after_2d_attention,w_dr1_esm_1b)+b_dr1_esm_1b)
                h_dr1_esm_1b=tflearn.batch_normalization(h_dr1_esm_1b)
        
            with tf.name_scope('esm_1b_fc_2'):
                w_dr2_esm_1b=weight_variable([256,256])
                b_dr2_esm_1b=bias_variable([256])
                h_dr2_esm_1b=tf.nn.relu(tf.matmul(h_dr1_esm_1b,w_dr2_esm_1b)+b_dr2_esm_1b)
                h_dr2_esm_1b=tflearn.batch_normalization(h_dr2_esm_1b)
    

        with tf.name_scope('densely_connected_layers'):
            with tf.name_scope('fc_1'):
                b_fc1=bias_variable([256])
                w_fc1_domain=weight_variable([256,256])
                w_fc1_esm_1b=weight_variable([256,256])

                w_shortCut_domain=weight_variable([16306,256])
                b_shortCut_domain=bias_variable([256])
                h_shortCut_domain=tf.nn.relu(tf.matmul(domain_after_2d_attention,w_shortCut_domain)+b_shortCut_domain)
                h_shortCut_domain=tflearn.batch_normalization(h_shortCut_domain)
                h_dr1_domain=tf.add(h_shortCut_domain,h_dr1_domain)  
                
                w_shortCut_esm_1b=weight_variable([1280,256])
                b_shortCut_esm_1b=bias_variable([256])
                h_shortCut_esm_1b=tf.nn.relu(tf.matmul(esm_1b_after_2d_attention,w_shortCut_esm_1b)+b_shortCut_esm_1b)
                h_shortCut_esm_1b=tflearn.batch_normalization(h_shortCut_esm_1b)
                h_dr2_esm_1b=tf.add(h_shortCut_esm_1b,h_dr2_esm_1b)    
                
                h_fc1=tflearn.prelu(
                    tf.matmul(h_dr1_domain,w_fc1_domain)+tf.matmul(h_dr2_esm_1b,w_fc1_esm_1b)+b_fc1)
    
                
                h_fc1=tflearn.batch_normalization(h_fc1)
                if DROPOUT==True:
                    h_fc3=tf.nn.dropout(h_fc3,keep_prob)
    
        #ADD SOFTMAX LAYER
        with tf.name_scope('softmax_layer'):
            w_fc4=weight_variable([256,len(unique_label)])
            b_fc4=bias_variable([len(unique_label)])
            y_conv_logit=tf.matmul(h_fc1,w_fc4)+b_fc4
            y_conv=tf.nn.softmax(y_conv_logit)
            ##normal softmax end


    #DEFINE EVALUATION
    with tf.name_scope('accuracy'):
        with tf.name_scope('correct_prediction'):
            predicted_label=tf.argmax(y_conv,1)



    sess.run(tf.global_variables_initializer())
    saver = tf.train.Saver()
    saver.restore(sess, model_name)



    def whole_set_check():
        predict_test_label=[]
        prob_out_list = []
        number_of_full_batch=int(math.floor(len(test_esm_1b)/batch_size))
        for i in range(number_of_full_batch):
            predicted_label_out, prob_out = sess.run([predicted_label, y_conv],
                feed_dict={domain: test_functional_domain_encoding[i*batch_size:(i+1)*batch_size], 
                esm_1b: test_esm_1b[i*batch_size:(i+1)*batch_size], keep_prob: 1.0})
            predicted_label_out_list = list(predicted_label_out)
            predict_test_label+=predicted_label_out_list
            prob_out_current_list = list(prob_out)
            prob_out_list+=prob_out_current_list

        predicted_label_out, prob_out = sess.run([predicted_label, y_conv],
            feed_dict={domain: test_functional_domain_encoding[number_of_full_batch*batch_size:],
            esm_1b: test_esm_1b[number_of_full_batch*batch_size:], keep_prob: 1.0})

        # pdb.set_trace()
        predicted_label_out_list = list(predicted_label_out)
        predict_test_label+=predicted_label_out_list
        prob_out_current_list = list(prob_out)
        prob_out_list+=prob_out_current_list

        return predict_test_label, prob_out_list



    predict_label, prob_list = whole_set_check()


    predict_digit = map(lambda x: unique_label[x], predict_label)
    return list(predict_digit), unique_label, prob_list








